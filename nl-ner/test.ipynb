{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments\n",
    "from transformers import DataCollatorForTokenClassification, Trainer\n",
    "from datasets import load_dataset, load_metric\n",
    "import numpy\n",
    "from json import load\n",
    "from os import getcwd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_auth_key(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        key = load(f)\n",
    "    return key[\"auth_key\"]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_token_path = f\"{getcwd()}\\\\..\\\\auth_key.json\"\n",
    "\n",
    "\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "dataset_name = \"cw1521/en-st-ner-small\"\n",
    "model_name = \"nl-ner-sm-10\"\n",
    "\n",
    "\n",
    "\n",
    "auth_token = get_auth_key(auth_token_path)\n",
    "output_path = f\"{getcwd()}\\\\output\\\\{model_name}\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_id_map = {\n",
    "    \"0\": \"O\",\n",
    "    \"1\": \"L-DEMO\",\n",
    "    \"2\": \"L-BA\",\n",
    "    \"3\": \"V-BA\",\n",
    "    \"4\": \"L-GROUND\",\n",
    "    \"5\": \"L-BALL\",\n",
    "    \"6\": \"L-SPEED\",\n",
    "    \"7\": \"V-SPEED\",\n",
    "    \"8\": \"L-DIR\",\n",
    "    \"9\": \"V-DIR\",\n",
    "    \"10\": \"L-BRAKE\",\n",
    "    \"11\": \"L-STEER\",\n",
    "    \"12\": \"V-STEER\",\n",
    "    \"13\": \"L-THROTTLE\",\n",
    "    \"14\": \"V-THROTTLE\",\n",
    "    \"15\": \"L-BOOST\",\n",
    "    \"16\": \"L-POS\"\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tag_map = {\n",
    "    \"O\": 0,\n",
    "    \"L-DEMO\": 1,\n",
    "    \"L-BA\": 2,\n",
    "    \"V-BA\": 3,\n",
    "    \"L-GROUND\": 4,\n",
    "    \"L-BALL\": 5,\n",
    "    \"L-SPEED\": 6,\n",
    "    \"V-SPEED\": 7,\n",
    "    \"L-DIR\": 8,\n",
    "    \"V-DIR\": 9,\n",
    "    \"L-BRAKE\": 10,\n",
    "    \"L-STEER\": 11,\n",
    "    \"V-STEER\": 12,\n",
    "    \"L-THROTTLE\": 13,\n",
    "    \"V-THROTTLE\": 14,\n",
    "    \"L-BOOST\": 15,\n",
    "    \"L-POS\": 16\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list =  [\n",
    "    \"O\",\n",
    "    \"L-DEMO\",\n",
    "    \"L-BA\",\n",
    "    \"V-BA\",\n",
    "    \"L-GROUND\",\n",
    "    \"L-BALL\",\n",
    "    \"L-SPEED\",\n",
    "    \"V-SPEED\",\n",
    "    \"L-DIR\",\n",
    "    \"V-DIR\",\n",
    "    \"L-BRAKE\",\n",
    "    \"L-STEER\",\n",
    "    \"V-STEER\",\n",
    "    \"L-THROTTLE\",\n",
    "    \"V-THROTTLE\",\n",
    "    \"L-BOOST\",\n",
    "    \"L-POS\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datafiles():\n",
    "    train = [\n",
    "    'oracle-train1.json',\n",
    "    'oracle-train2.json',\n",
    "    'oracle-train3.json',\n",
    "    'oracle-train4.json',\n",
    "    'oracle-train5.json',\n",
    "    'oracle-train6.json',\n",
    "    'oracle-train7.json',\n",
    "    'oracle-train8.json',\n",
    "    'oracle-train9.json',\n",
    "    'oracle-train10.json'\n",
    "    ]   \n",
    "\n",
    "    valid = ['oracle-valid.json']\n",
    "    return train, valid\n",
    "\n",
    "\n",
    "\n",
    "def get_dataset(name):\n",
    "    train, valid = get_datafiles()\n",
    "    return load_dataset(    \n",
    "        name,\n",
    "        data_files={'train':train, 'valid':valid},\n",
    "        use_auth_token=auth_token,\n",
    "        field=\"data\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\huggingface_hub-0.10.0rc3-py3.8.egg\\huggingface_hub\\utils\\_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'dataset_info': token. Will not be supported from version '0.12'.\n",
      "Using custom data configuration cw1521--en-st-ner-small-5590803a90d98b46\n",
      "Found cached dataset json (C:/Users/school/.cache/huggingface/datasets/cw1521___json/cw1521--en-st-ner-small-5590803a90d98b46/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0fae6efdb34a0b89877ca6d3cf6e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = get_dataset(dataset_name)\n",
    "train, valid = get_datafiles()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print Structure of Dataset and Number of Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure of Dataset:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['ner_sentence', 'ner_tags', 'sentence', 'state'],\n",
      "        num_rows: 865330\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['ner_sentence', 'ner_tags', 'sentence', 'state'],\n",
      "        num_rows: 123620\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['ner_sentence', 'ner_tags', 'sentence', 'state'],\n",
      "        num_rows: 123620\n",
      "    })\n",
      "})\n",
      "Total number of elements in dataset: 1112570\n"
     ]
    }
   ],
   "source": [
    "print(f\"Structure of Dataset:\\n{dataset}\")\n",
    "print(f'Total number of elements in dataset: {len(dataset[\"train\"]) + len(dataset[\"test\"]) + len(dataset[\"validation\"])}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Data Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example data item:\n",
      "{'ner_sentence': 'quadrant 3 blue goal south wall currenly 48 boost currently travelling north pressed brakes on ground speed 618 miles per hour', 'ner_tags': [0, 0, 16, 16, 0, 0, 16, 16, 0, 0, 0, 16, 16, 0, 0, 2, 0, 3, 0, 2, 0, 0, 8, 8, 9, 0, 0, 10, 0, 10, 0, 0, 0, 0, 4, 0, 4, 0, 0, 0, 6, 0, 7, 6, 6, 6, 0], 'sentence': \"I'm in quadrant 3 near the blue goal and near the south wall. I currenly have 48 percent boost. I'm currently travelling north. I pressed the brakes. My car is on the ground. My current speed is 618 miles per hour.\", 'state': 'position -188 -4789 17 direction 75 on_ground True is_demoed False ball_touched False boost_amount 48 speed 618 throttle 0 steer 0 jump 1 boost 0 handbrake 1'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Example data item:\\n{dataset['train'][0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Tokenizer, Model, and Data Collator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "471198e62b0243989bdbd50443cfdc74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(ner_id_map))\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3404725f230c4b8ca4f5cccc457c47ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/866 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeee75f1ff7b4c20a5376d7a8a716698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"sentence\"], truncation=True)\n",
    "    tokenized_inputs[\"labels\"] = examples[\"ner_tags\"]\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "train_tokenized_datasets = dataset[\"train\"].map(tokenize_and_align_labels, batched=True)\n",
    "valid_tokenized_datasets = dataset[\"validation\"].map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\school\\AppData\\Local\\Temp\\ipykernel_11444\\2850195878.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83fed39dc6be45ca990c82a950235c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [[label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "    true_labels = [[label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"]}\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer Arguments and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_args(num_epochs):\n",
    "    batch_size = 64\n",
    "    args = TrainingArguments(\n",
    "        model_name,\n",
    "        save_steps=50,\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        learning_rate=1e-4,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        weight_decay=1e-5,\n",
    "        save_total_limit=3,\n",
    "        num_train_epochs=num_epochs,\n",
    "        logging_dir='./logs',\n",
    "    \tgradient_accumulation_steps=4,\n",
    "\t    tf32=True\n",
    "    )\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "args = get_training_args(10)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_tokenized_datasets,\n",
    "    eval_dataset=valid_tokenized_datasets,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, Evaluate, Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: state, sentence, ner_tags, ner_sentence. If state, sentence, ner_tags, ner_sentence are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 865330\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 33800\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d75c2bdd13b468cb6da99395b2b8ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\school\\thesis\\code\\language-module\\nl-ner\\test.ipynb Cell 27\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/school/thesis/code/language-module/nl-ner/test.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/school/thesis/code/language-module/nl-ner/test.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39mevaluate()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/school/thesis/code/language-module/nl-ner/test.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m trainer\u001b[39m.\u001b[39msave_model()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\trainer.py:1521\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1518\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1519\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1520\u001b[0m )\n\u001b[1;32m-> 1521\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1522\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1523\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1524\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1525\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1526\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\trainer.py:1763\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1761\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1762\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1763\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1765\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1766\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1767\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1768\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1769\u001b[0m ):\n\u001b[0;32m   1770\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1771\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\trainer.py:2517\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2515\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed\u001b[39m.\u001b[39mbackward(loss)\n\u001b[0;32m   2516\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2517\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m   2519\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
