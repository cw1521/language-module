{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments\n",
    "from transformers import DataCollatorForTokenClassification, Trainer\n",
    "from datasets import load_dataset, load_metric\n",
    "import numpy\n",
    "from json import load\n",
    "from os import getcwd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_auth_key(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        key = load(f)\n",
    "    return key[\"auth_key\"]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_token_path = f\"{getcwd()}\\\\..\\\\auth_key.json\"\n",
    "\n",
    "\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "dataset_name = \"cw1521/en-st-ner-small\"\n",
    "model_name = \"nl-ner-sm-10\"\n",
    "\n",
    "\n",
    "\n",
    "auth_token = get_auth_key(auth_token_path)\n",
    "output_path = f\"{getcwd()}\\\\output\\\\{model_name}\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_id_map = {\n",
    "    \"0\": \"O\",\n",
    "    \"1\": \"L-DEMO\",\n",
    "    \"2\": \"L-BA\",\n",
    "    \"3\": \"V-BA\",\n",
    "    \"4\": \"L-GROUND\",\n",
    "    \"5\": \"L-BALL\",\n",
    "    \"6\": \"L-SPEED\",\n",
    "    \"7\": \"V-SPEED\",\n",
    "    \"8\": \"L-DIR\",\n",
    "    \"9\": \"V-DIR\",\n",
    "    \"10\": \"L-BRAKE\",\n",
    "    \"11\": \"L-STEER\",\n",
    "    \"12\": \"V-STEER\",\n",
    "    \"13\": \"L-THROTTLE\",\n",
    "    \"14\": \"V-THROTTLE\",\n",
    "    \"15\": \"L-BOOST\",\n",
    "    \"16\": \"L-POS\"\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tag_map = {\n",
    "    \"O\": 0,\n",
    "    \"L-DEMO\": 1,\n",
    "    \"L-BA\": 2,\n",
    "    \"V-BA\": 3,\n",
    "    \"L-GROUND\": 4,\n",
    "    \"L-BALL\": 5,\n",
    "    \"L-SPEED\": 6,\n",
    "    \"V-SPEED\": 7,\n",
    "    \"L-DIR\": 8,\n",
    "    \"V-DIR\": 9,\n",
    "    \"L-BRAKE\": 10,\n",
    "    \"L-STEER\": 11,\n",
    "    \"V-STEER\": 12,\n",
    "    \"L-THROTTLE\": 13,\n",
    "    \"V-THROTTLE\": 14,\n",
    "    \"L-BOOST\": 15,\n",
    "    \"L-POS\": 16\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list =  [\n",
    "    \"O\",\n",
    "    \"L-DEMO\",\n",
    "    \"L-BA\",\n",
    "    \"V-BA\",\n",
    "    \"L-GROUND\",\n",
    "    \"L-BALL\",\n",
    "    \"L-SPEED\",\n",
    "    \"V-SPEED\",\n",
    "    \"L-DIR\",\n",
    "    \"V-DIR\",\n",
    "    \"L-BRAKE\",\n",
    "    \"L-STEER\",\n",
    "    \"V-STEER\",\n",
    "    \"L-THROTTLE\",\n",
    "    \"V-THROTTLE\",\n",
    "    \"L-BOOST\",\n",
    "    \"L-POS\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datafiles():\n",
    "    train = [\n",
    "    'oracle-train1.json',\n",
    "    'oracle-train2.json',\n",
    "    'oracle-train3.json',\n",
    "    'oracle-train4.json',\n",
    "    'oracle-train5.json',\n",
    "    'oracle-train6.json',\n",
    "    'oracle-train7.json',\n",
    "    'oracle-train8.json',\n",
    "    'oracle-train9.json',\n",
    "    'oracle-train10.json'\n",
    "    ]   \n",
    "\n",
    "    valid = ['oracle-valid.json']\n",
    "    return train, valid\n",
    "\n",
    "\n",
    "\n",
    "def get_dataset(name):\n",
    "    train, valid = get_datafiles()\n",
    "    return load_dataset(    \n",
    "        name,\n",
    "        data_files={'train':train, 'valid':valid},\n",
    "        use_auth_token=auth_token,\n",
    "        field=\"data\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\huggingface_hub-0.10.0rc3-py3.8.egg\\huggingface_hub\\utils\\_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'dataset_info': token. Will not be supported from version '0.12'.\n",
      "Using custom data configuration cw1521--en-st-ner-small-5590803a90d98b46\n",
      "Found cached dataset json (C:/Users/school/.cache/huggingface/datasets/cw1521___json/cw1521--en-st-ner-small-5590803a90d98b46/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e461d48d8e3949049c1089451d825363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = get_dataset(dataset_name)\n",
    "train, valid = get_datafiles()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print Structure of Dataset and Number of Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure of Dataset:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['ner_sentence', 'ner_tags', 'sentence', 'state'],\n",
      "        num_rows: 865330\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['ner_sentence', 'ner_tags', 'sentence', 'state'],\n",
      "        num_rows: 123620\n",
      "    })\n",
      "})\n",
      "Total number of elements in dataset: 988950\n"
     ]
    }
   ],
   "source": [
    "print(f\"Structure of Dataset:\\n{dataset}\")\n",
    "print(f'Total number of elements in dataset: {len(dataset[\"train\"]) + len(dataset[\"valid\"])}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Data Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example data item:\n",
      "{'ner_sentence': 'quadrant 3 blue goal south wall currenly 48 boost currently travelling north pressed brakes on ground speed 618 miles per hour', 'ner_tags': [0, 0, 16, 16, 0, 0, 16, 16, 0, 0, 0, 16, 16, 0, 0, 2, 0, 3, 0, 2, 0, 0, 8, 8, 9, 0, 0, 10, 0, 10, 0, 0, 0, 0, 4, 0, 4, 0, 0, 0, 6, 0, 7, 6, 6, 6, 0], 'sentence': \"I'm in quadrant 3 near the blue goal and near the south wall. I currenly have 48 percent boost. I'm currently travelling north. I pressed the brakes. My car is on the ground. My current speed is 618 miles per hour.\", 'state': 'position -188 -4789 17 direction 75 on_ground True is_demoed False ball_touched False boost_amount 48 speed 618 throttle 0 steer 0 jump 1 boost 0 handbrake 1'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Example data item:\\n{dataset['train'][0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Tokenizer, Model, and Data Collator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d8e3019fbd4f2392f7c6c6db9282c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36fd250b48394969aa0a406739280845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45db9b2131842efafc5d50ef3b80a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd6b1decc844528ac87a26fa563728f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d09978fc0f4c25ac494575e3e47b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(ner_id_map))\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42a2e9ce8954a4181a1d37319b7c1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/866 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "815329f7eebf41de9228a181f8b49146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"sentence\"], truncation=True)\n",
    "    tokenized_inputs[\"labels\"] = examples[\"ner_tags\"]\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "train_tokenized_datasets = dataset[\"train\"].map(tokenize_and_align_labels, batched=True)\n",
    "valid_tokenized_datasets = dataset[\"valid\"].map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\school\\AppData\\Local\\Temp\\ipykernel_3584\\2850195878.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [[label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "    true_labels = [[label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"]}\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer Arguments and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_args(num_epochs):\n",
    "    batch_size = 64\n",
    "    args = TrainingArguments(\n",
    "        model_name,\n",
    "        save_steps=50,\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        learning_rate=1e-4,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        weight_decay=1e-5,\n",
    "        save_total_limit=3,\n",
    "        num_train_epochs=num_epochs,\n",
    "        logging_dir='./logs',\n",
    "    \tgradient_accumulation_steps=4,\n",
    "\t    tf32=True\n",
    "    )\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "args = get_training_args(10)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_tokenized_datasets,\n",
    "    eval_dataset=valid_tokenized_datasets,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, Evaluate, Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner_tags, ner_sentence, state, sentence. If ner_tags, ner_sentence, state, sentence are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 865330\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 540830\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc21dd9faa5344269b77044bf469e7be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540830 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
