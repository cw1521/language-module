{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in c:\\python310\\lib\\site-packages (4.30.2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1.2\n",
      "[notice] To update, run: C:\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: datasets in c:\\python310\\lib\\site-packages (2.13.1)\n",
      "Requirement already satisfied: tokenizers in c:\\python310\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: seqeval in c:\\users\\school\\appdata\\roaming\\python\\python310\\site-packages (1.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\school\\appdata\\roaming\\python\\python310\\site-packages (from transformers[torch]) (21.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\python310\\lib\\site-packages (from transformers[torch]) (0.3.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\python310\\lib\\site-packages (from transformers[torch]) (1.25.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\python310\\lib\\site-packages (from transformers[torch]) (0.16.3)\n",
      "Requirement already satisfied: requests in c:\\python310\\lib\\site-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\python310\\lib\\site-packages (from transformers[torch]) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\python310\\lib\\site-packages (from transformers[torch]) (4.64.1)\n",
      "Requirement already satisfied: filelock in c:\\python310\\lib\\site-packages (from transformers[torch]) (3.12.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\python310\\lib\\site-packages (from transformers[torch]) (2023.6.3)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.9 in c:\\python310\\lib\\site-packages (from transformers[torch]) (1.12.1+cu116)\n",
      "Requirement already satisfied: accelerate>=0.20.2 in c:\\python310\\lib\\site-packages (from transformers[torch]) (0.20.3)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\python310\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\python310\\lib\\site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: pandas in c:\\python310\\lib\\site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: multiprocess in c:\\python310\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in c:\\python310\\lib\\site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\python310\\lib\\site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: xxhash in c:\\python310\\lib\\site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\school\\appdata\\roaming\\python\\python310\\site-packages (from seqeval) (1.3.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\school\\appdata\\roaming\\python\\python310\\site-packages (from accelerate>=0.20.2->transformers[torch]) (5.9.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\school\\appdata\\roaming\\python\\python310\\site-packages (from packaging>=20.0->transformers[torch]) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests->transformers[torch]) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests->transformers[torch]) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\school\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\school\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\school\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\school\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python310\\lib\\site-packages (from pandas->datasets) (2022.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\python310\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\school\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\school\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[torch] datasets tokenizers seqeval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments\n",
    "from transformers import DataCollatorForTokenClassification, Trainer\n",
    "from datasets import load_dataset, load_metric\n",
    "import numpy as np\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "dataset_name = \"cw1521/nl-st\"\n",
    "model_name = \"nl-ner-sm-10\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\huggingface_hub-0.10.0rc3-py3.8.egg\\huggingface_hub\\utils\\_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'dataset_info': token. Will not be supported from version '0.12'.\n",
      "Using custom data configuration cw1521--nl-st-0535027a99994970\n",
      "Found cached dataset json (C:/Users/school/.cache/huggingface/datasets/cw1521___json/cw1521--nl-st-0535027a99994970/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e08e7be4524b668239ecfb04c469a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"cw1521/nl-st\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print Structure of Dataset and Number of Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure of Dataset:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['ner_sentence', 'ner_tags', 'sentence', 'state'],\n",
      "        num_rows: 865320\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['ner_sentence', 'ner_tags', 'sentence', 'state'],\n",
      "        num_rows: 123615\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['ner_sentence', 'ner_tags', 'sentence', 'state'],\n",
      "        num_rows: 247230\n",
      "    })\n",
      "})\n",
      "Total number of elements in dataset: 1112550\n"
     ]
    }
   ],
   "source": [
    "print(f\"Structure of Dataset:\\n{dataset}\")\n",
    "print(f'Total number of elements in dataset: {len(dataset[\"train\"]) + len(dataset[\"validation\"])}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Data Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example data item:\n",
      "{'ner_sentence': 'speed 1237 currently braking boost 23 direction east quadrant 2 east wall', 'ner_tags': ['O', 'O', 'L-SPEED', 'O', 'V-SPEED', 'O', 'O', 'L-BRAKE', 'L-BRAKE', 'O', 'O', 'L-BA', 'L-BA', 'O', 'V-BA', 'O', 'O', 'L-DIR', 'L-DIR', 'O', 'V-DIR', 'O', 'O', 'O', 'L-POS', 'L-POS', 'O', 'O', 'L-POS', 'L-POS', 'O'], 'sentence': \"My current speed is 1237. I'M currently braking. My current boost is 23. My current direction is east. I'm in quadrant 2 near the east wall.\", 'state': 'is_demoed False ball_touched False boost_amount 23 position -4048 3552 239 direction 19 speed 1237 on_ground False throttle 0 steer 0 jump 1 boost 0 handbrake 1'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Example data item:\\n{dataset['train'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoding_dict = {\n",
    "    \"O\": 0,\n",
    "    \"L-DEMO\": 1,\n",
    "    \"L-BA\": 2,\n",
    "    \"V-BA\": 3,\n",
    "    \"L-GROUND\": 4,\n",
    "    \"L-BALL\": 5,\n",
    "    \"L-SPEED\": 6,\n",
    "    \"V-SPEED\": 7,\n",
    "    \"L-DIR\": 8,\n",
    "    \"V-DIR\": 9,\n",
    "    \"L-BRAKE\": 10,\n",
    "    \"L-STEER\": 11,\n",
    "    \"V-STEER\": 12,\n",
    "    \"L-THROTTLE\": 13,\n",
    "    \"V-THROTTLE\": 14,\n",
    "    \"L-BOOST\": 15,\n",
    "    \"L-POS\": 16\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\n",
    "    \"O\",\n",
    "    \"L-DEMO\",\n",
    "    \"L-BA\",\n",
    "    \"V-BA\",\n",
    "    \"L-GROUND\",\n",
    "    \"L-BALL\",\n",
    "    \"L-SPEED\",\n",
    "    \"V-SPEED\",\n",
    "    \"L-DIR\",\n",
    "    \"V-DIR\",\n",
    "    \"L-BRAKE\",\n",
    "    \"L-STEER\",\n",
    "    \"V-STEER\",\n",
    "    \"L-THROTTLE\",\n",
    "    \"V-THROTTLE\",\n",
    "    \"L-BOOST\",\n",
    "    \"L-POS\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Tokenizer, Model, and Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "max_input = 512\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, model_max_length=max_input)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_encoding_dict))\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ner_sentence', 'ner_tags', 'sentence', 'state'],\n",
       "        num_rows: 865320\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ner_sentence', 'ner_tags', 'sentence', 'state'],\n",
       "        num_rows: 123615\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ner_sentence', 'ner_tags', 'sentence', 'state'],\n",
       "        num_rows: 247230\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns([\"state\", \"ner_sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    return sentence.replace(\".\", \" .\").replace(\"!\", \" !\").split(\" \")\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    label_all_tokens = True\n",
    "    tokenized_inputs = tokenizer(\n",
    "        list(map(tokenize_sentence, examples[\"sentence\"])),\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif label[word_idx] == 'O':\n",
    "                label_ids.append(0)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_encoding_dict[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(label_encoding_dict[label[word_idx]] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "        \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:/Users/school/.cache/huggingface/datasets/cw1521___json/cw1521--nl-st-0535027a99994970/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab\\cache-021e10c334392bab.arrow\n"
     ]
    }
   ],
   "source": [
    "train = dataset[\"train\"].map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=[\"sentence\", \"ner_tags\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93482ab94d2436c86e44db11ed078e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/248 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "valid = dataset[\"validation\"].map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=[\"sentence\", \"ner_tags\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 247230\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(valid[\"input_ids\"][0]), len(valid[\"labels\"][0]), len(valid[\"attention_mask\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\school\\AppData\\Local\\Temp\\ipykernel_31272\\2850195878.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [[label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "    true_labels = [[label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"]}\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer Arguments and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_args(num_epochs):\n",
    "    batch_size = 8\n",
    "    args = TrainingArguments(\n",
    "        model_name,\n",
    "        save_steps=50,\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        learning_rate=1e-4,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        weight_decay=1e-5,\n",
    "        save_total_limit=3,\n",
    "        num_train_epochs=num_epochs,\n",
    "        logging_dir='./logs',\n",
    "    \tgradient_accumulation_steps=4,\n",
    "\t    fp16=True\n",
    "    )\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "args = get_training_args(1)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=valid,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 865320\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 27041\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24effc1090b14db6b8e6a30364e362fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Saving model checkpoint to nl-ner-sm-10\\checkpoint-50\n",
      "Configuration saved in nl-ner-sm-10\\checkpoint-50\\config.json\n",
      "Model weights saved in nl-ner-sm-10\\checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in nl-ner-sm-10\\checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in nl-ner-sm-10\\checkpoint-50\\special_tokens_map.json\n",
      "Saving model checkpoint to nl-ner-sm-10\\checkpoint-100\n",
      "Configuration saved in nl-ner-sm-10\\checkpoint-100\\config.json\n",
      "Model weights saved in nl-ner-sm-10\\checkpoint-100\\pytorch_model.bin\n",
      "tokenizer config file saved in nl-ner-sm-10\\checkpoint-100\\tokenizer_config.json\n",
      "Special tokens file saved in nl-ner-sm-10\\checkpoint-100\\special_tokens_map.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\school\\thesis\\code\\language-module\\ner\\test-ner.ipynb Cell 33\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/school/thesis/code/language-module/ner/test-ner.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\trainer.py:1521\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1518\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1519\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1520\u001b[0m )\n\u001b[1;32m-> 1521\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1522\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1523\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1524\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1525\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1526\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\trainer.py:1763\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1761\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1762\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1763\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1765\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1766\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1767\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1768\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1769\u001b[0m ):\n\u001b[0;32m   1770\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1771\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\trainer.py:2509\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2506\u001b[0m     loss \u001b[39m=\u001b[39m loss \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n\u001b[0;32m   2508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_grad_scaling:\n\u001b[1;32m-> 2509\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mscale(loss)\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m   2510\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_apex:\n\u001b[0;32m   2511\u001b[0m     \u001b[39mwith\u001b[39;00m amp\u001b[39m.\u001b[39mscale_loss(loss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer) \u001b[39mas\u001b[39;00m scaled_loss:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
